+++
author = "AWS Kubernetes Developer Advocates"
categories = ["Archive", "2022", "Weekly"]
date = 2022-06-17T07:00:00Z
description = "Using Styra DAS Free and OPA with EKS, Amazon EKS single sign-on using AWS SSO, GitOps, K9s, and more"
draft = false
slug = "021"
title = "EKS News 021"
+++

{{< eo >}}

Editor's Note: There will not be an EKS News next week due to the author's being at [Open Source Summit](https://events.linuxfoundation.org/open-source-summit-north-america/). If you're in Austin next week, swing by the AWS booth and say hello.

In this issue, we'll talking about using Styra DAS Free and OPA with EKS, Amazon EKS single sign-on using AWS SSO, GitOps, eBPF and the future of sidecars, and more.

## New and notable blogs

[Harden Amazon EKS in minutes with Styra DAS Free and OPA](https://aws.amazon.com/blogs/containers/harden-amazon-eks-in-minutes-styra-das-free-and-opa/)

* In the [Amazon EKS Best Practices Guide](https://aws.github.io/aws-eks-best-practices/), AWS recommends Open Policy Agent (OPA) as a policy-as-code (PaC) solution for Kubernetes pod security
* This blog walks you through your first five policies in five minutes
  * Restrict the containers that can run as privileged
  * Configure read-only file system
  * Don't allow any containers to run as root
  * Disallow privilege escalation
  * Set requests and limits for each container
* [Styra DAS Free](https://aws.amazon.com/marketplace/pp/prodview-4qemlqp6lodwg) is the same solution that's been proven in some of the largest Amazon EKS and Kubernetes deployments in the world, running in production at global enterprises like Capital One, the European Patent Office, and Zalando

[A quick path to Amazon EKS single sign-on using AWS SSO](https://aws.amazon.com/blogs/containers/a-quick-path-to-amazon-eks-single-sign-on-using-aws-sso/)

* [AWS Identity and Access Management (IAM)](https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html) and [Kubernetes role-based access control (RBAC)](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) provide the tools to build a strong least-privilege security posture
* In this blog, we demonstrate a quick and direct procedure to implement single sign-on to access Kubernetes resources running on your [Amazon EKS](https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html) clusters
* You can use [AWS Single Sign-On (AWS SSO)](https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html) in combination with Kubernetes RBAC to manage access using [AWS Command Line Interface (AWS CLI)](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html) and other Kubernetes CLI tools
* Follow the steps below to get up and running quickly, based on AWS best practices, and you can customize them according to your specific needs

## Ecosystem News

[Kubernetes Workload Identity with AWS SDK for Go v2](https://blog.jimmyray.io/kubernetes-workload-identity-with-aws-sdk-for-go-v2-927d2f258057)

* When using [Amazon Elastic Kubernetes Service (EKS)](https://aws.amazon.com/eks/), [IAM Roles for Service Accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-technical-overview.html), a.k.a. IRSA, is used to grant identity and permissions using [AWS Identity and Access Management (IAM)](https://aws.amazon.com/iam/)
* To use this federated-identities feature with EKS, an [IAM OIDC provider must be associated with the cluster](https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html)
* The AWS IAM Roles for Service Accounts (IRSA) feature eases the application of IAM principals to pods via service account annotations and the EKS Pod Identity Webhook
* The AWS SDK for Go v2 uses the provided environment variables and Kubernetes projected volumes to gain IAM credentials via a federated and managed web identity provided as a JSON Web Token (JWT)

[Dynamic Kubernetes Cluster Scaling at Airbnb](https://medium.com/airbnb-engineering/dynamic-kubernetes-cluster-scaling-at-airbnb-d79ae3afa132)  

* An important part of running Airbnb's infrastructure is ensuring our cloud spending automatically scales with demand, both up and down
* In this post, we'll talk about how we dynamically size our clusters using the Kubernetes Cluster Autoscaler
* Having the largest portion of compute at Airbnb on a single platform provided a strong, consolidated lever to improve efficiency, and we are now focused on generalizing our cluster setup (think "[cattle, not pets](http://cloudscaling.com/blog/cloud-computing/the-history-of-pets-vs-cattle/)")

[Manage Your AWS Resources from Kubernetes with ACK](https://eminalemdar.medium.com/manage-your-aws-resources-from-kubernetes-with-ack-3cf06a4b0770)

* [AWS Controllers for Kubernetes (ACK)](https://aws-controllers-k8s.github.io/community/) allows you to manage AWS Services directly from your Kubernetes cluster with plain YAML files
* There are multiple Controllers for different AWS Services and these are called Service Controllers
* Contributors are working really hard to extend the supported services list in ACK
* "With the help of tools like [ACK](https://aws-controllers-k8s.github.io/community/) our life is getting easier"

[Debugging Kubernetes Pods: Deep Dive](https://medium.com/better-programming/debugging-kubernetes-pods-deep-dive-d6b2814cd8ce)

* In this article, I will talk about debugging and troubleshooting Kubernetes pods using ephemeral containers
* [Ephemeral containers](https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/) are useful for interactive troubleshooting when `kubectl exec` is insufficient because a container has crashed or a container image doesn't include debugging utilities, such as distroless images, or the running pods don't have the required privileges for debugging
* The main idea behind ephemeral containers is that K8S adds a new container with a selected custom image to an existing pod without the need for restarting this pod
* This enables a number of troubleshooting options including networking, tracing/profiling processes, and debugging via a shell on the node

[Scaling Container Technologies at Coinbase with Kubernetes](https://blog.coinbase.com/scaling-container-technologies-at-coinbase-with-kubernetes-de18efa9389f)

* Our recent evaluation of Kubernetes underscored its suitability for scaling Coinbase into the future
* We've now concluded that managed Kubernetes offerings reduce this operational burden without compromising our stack security
* Managed Kubernetes offerings, such as AWS EKS, take on the responsibility of operating, maintaining, and securing the control plane, reducing the operational burden of running many clusters
* Reducing our operational burden and security responsibility enables us to focus on building the orchestration and automation that is required to support many clusters across a large engineering organization

[eBPF, sidecars, and the future of the service mesh](https://buoyant.io/2022/06/07/ebpf-sidecars-and-the-future-of-the-service-mesh/)

* Could [eBPF](https://ebpf.io/) allow us to replace Linkerd's sidecars proxies entirely, and just do everything in the kernel?
* The kernel must ensure that no program can stop or break another program, or deny it resources, or interfere with its ability to run, or read its data from memory, or the network, or disk, unless given explicit permission to do so
* Kubernetes's gift to the world is a composable platform with clear boundaries between layers, and the relationship between eBPF and service meshes fits right into that model: the CNI is responsible for L3/L4 traffic, and the service mesh for L7.
* Any eBPF service mesh still requires proxies and these proxies are typically sidecars
* Could eBPF enable a per-host proxy? Yes, but compared to sidecars, per-host proxies are worse for operations, worse for maintenance, and worse for security (the blast radius becomes the entire host versus one pod).

[Updated: Apache Log4j2 CVE-2021-44228 node agent](https://github.com/aws-samples/kubernetes-log4j-cve-2021-44228-node-agent)

* There is a new version of the patch that addresses the vulnerabilities described in the following security bulletins:
  * [ALAS2-2022-1806](https://alas.aws.amazon.com/AL2/ALAS-2022-1806.html)
  * [ALAS-2022-1601](https://alas.aws.amazon.com/ALAS-2022-1601.html)

[CVE-2021-25748: Ingress-nginx `path` sanitization can be bypassed with newline character · Issue #8686 · kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx/issues/8686)

* "A security issue was discovered in ingress-nginx where a user that can create or update ingress objects can use a newline character to bypass the sanitization of the spec.rules[].http.paths[].path field of an Ingress object (in the networking.k8s.io or extensions API group) to obtain the credentials of the ingress-nginx controller. In the default configuration, that credential has access to all secrets in the cluster."
* "This bug affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running `kubectl get po -n ingress-nginx`"
* "If you are unable to roll out the fix, this vulnerability can be mitigated by implementing an admission policy that restricts the spec.rules[].http.paths[].path field on the networking.k8s.io/Ingress resource to known safe characters (see the newly added [rules](https://github.com/kubernetes/ingress-nginx/blame/main/internal/ingress/inspector/rules.go), or the suggested value for [annotation-value-word-blocklist](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#annotation-value-word-blocklist))."

[Cloud Native Islamabad on LinkedIn: #kyverno #policyascode #k8s](https://www.linkedin.com/feed/update/urn:li:activity:6942726553889837056/)

* A Kyverno policy to trigger AWS Karpenter provisioners for auto-scaling different workloads

[Canadian internet outage attributed to beaver](https://therecord.media/canadian-internet-outage-beaver/)

* Because it's Friday!
* "Some residents of northwestern Canada lost network coverage for eight hours last week in an outage that has since been attributed to nature's architect: the beaver."
* "It's unusual, but it does happen every once in a while," Bob Gammer told [CTV](https://bc.ctvnews.ca/single-beaver-caused-mass-internet-cell-service-outages-in-northern-b-c-1.5944697
* "So I wouldn't be a rich man if I had a nickel for every beaver outage, but they do happen."
* A similar incident occurred in April of last year, DatacenterDynamics [reported](https://www.datacenterdynamics.com/en/news/beaver-causes-internet-outage-in-a-uniquely-canadian-turn-of-events/), when a beaver chewed through a Telus cable and used related material in its dam — temporarily downing network coverage for 900 residents in the process